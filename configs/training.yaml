# Shared training config
seed: 42
device: mps  # or cpu, cuda

encoder:
  text_model: sentence-transformers/all-MiniLM-L6-v2
  freeze_text_encoder: true
  type_embed_dim: 32
  d_event: 256

state_updater:
  type: gru  # or mamba (later)
  d_state: 512
  num_layers: 2
  dropout: 0.1

action_head:
  hidden_dim: 256
  dropout: 0.2

retriever:
  d_chunk: 384  # Match MiniLM embedding dim
  top_k: 5

phase2:
  num_negatives: 16
  margin: 0.2
  freeze_state_model: true
  train_query_proj_only: true

optimizer:
  type: adamw
  lr: 0.0001
  weight_decay: 0.01
  warmup_steps: 100

training:
  batch_size: 16
  max_epochs: 20
  early_stopping_patience: 5
  gradient_clip: 1.0
  log_every: 50
  eval_every: 500
